import { NextApiRequest, NextApiResponse } from 'next';
import { GoogleGenAI } from "@google/genai";

// ç’°å¢ƒåˆ¤å®šä»˜ãã‚­ãƒ£ãƒƒã‚·ãƒ¥
let kv: any;
try {
  kv = require('@vercel/kv').kv;
} catch (error) {
  console.log('Vercel KV not available');
}

const ai = new GoogleGenAI({
  apiKey: process.env.GEMINI_API_KEY
});

const CACHE_KEY = 'gemini_daily_message';
const CACHE_DURATION = 3600; // 1æ™‚é–“

// å®šæœŸå®Ÿè¡Œç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ
async function generateFreshMessage(): Promise<string> {
  const departureStation = process.env.DEPARTURE_STATION || "æ–°å®¿é§…";
  const arrivalStation = process.env.ARRIVAL_STATION || "æ¸‹è°·é§…";
  
  const prompts = [
    "ä»Šæ—¥ã®åŠ´ã„ä¸€è¨€ã‚’20æ–‡å­—ä»¥å†…ã§",
    "ä»•äº‹çµ‚äº†ã®é”æˆæ„Ÿã‚’15æ–‡å­—ã§è¡¨ç¾",
    `${departureStation}â†’${arrivalStation}ã®ãƒ«ãƒ¼ãƒˆæ¦‚è¦ã‚’25æ–‡å­—ã§`,
    "å¸°å®…æ™‚ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’20æ–‡å­—ã§",
    "æ˜æ—¥ã¸ã®åŠ±ã¾ã—ã‚’15æ–‡å­—ã§"
  ];

  const selectedPrompt = prompts[Math.floor(Math.random() * prompts.length)];

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: selectedPrompt,
    config: {
      thinkingConfig: {
        thinkingBudget: 0,
      },
    }
  });

  // å®‰å…¨ãªæ–‡å­—åˆ—å–å¾—ã¨nullãƒã‚§ãƒƒã‚¯
  const text = response?.text || response?.candidates?.[0]?.content?.parts?.[0]?.text;
  
  if (!text || typeof text !== 'string' || !text.trim()) {
    throw new Error('Invalid response from Gemini API');
  }
  
  return text.trim();
}

export default async function handler(req: NextApiRequest, res: NextApiResponse) {
  // Cron Jobèªè¨¼ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ï¼‰
  if (req.headers.authorization !== `Bearer ${process.env.CRON_SECRET}`) {
    return res.status(401).json({ error: 'Unauthorized' });
  }

  try {
    console.log('ğŸ”„ Warming cache...');
    
    // æ–°ã—ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ
    const freshMessage = await generateFreshMessage();
    
    // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æœ‰åŠ¹æ€§ã‚’å†ç¢ºèª
    if (!freshMessage || !freshMessage.trim()) {
      throw new Error('Generated message is empty');
    }
    
    // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    if (kv) {
      await kv.setex(CACHE_KEY, CACHE_DURATION, freshMessage);
      console.log('âœ… Cache warmed with fresh message');
    } else {
      console.log('âš ï¸ Vercel KV not available, skipping cache update');
    }
    
    return res.status(200).json({ 
      success: true, 
      message: 'Cache warmed successfully',
      preview: freshMessage.substring(0, 50) + (freshMessage.length > 50 ? '...' : ''),
      messageLength: freshMessage.length
    });

  } catch (error: any) {
    console.error('âŒ Cache warming failed:', error);
    return res.status(500).json({ 
      error: 'Cache warming failed',
      details: error.message 
    });
  }
} 